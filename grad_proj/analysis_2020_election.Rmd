---
title: "JOUR772 Grad Assignment"
author: "Aidan Hughes"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidycensus)
library(janitor)
library(stringi)
```

# Summary of project

I've chosen to work with the Maryland International Migration data set from the open data portal. This data set covers 2001-2021 and includes county-level "foreign-born international migration, net movement to/ from Puerto Rico, net Armed Forces movement and native emigration." The source is described as being "from the Population Division, U.S. Census Bureau, March 2022" and was provided on the portal by the Maryland Department of Planning. I've put a call in to the DoP to get more information on the Census data they apparently used to create this aggregate data set so I can replicate and/or modify their calculations (eg I would probably take troop movement out of the mix for the purposes of this analysis), but I wasn't able to reach anybody before the deadline for this assignment.

I've combined that data set with three other county-level data sets pertaining to election results in MD, population/racial data from the ACS, and overdose data from the state department of health.

I also reached out to Maryland’s Opioid Operational Command Center and requested OD data at any geographic level more granular than county, but they said county-level is the lowest they have data for. They said I could try asking the Vital Statistics Administration directly, but he wasn't sure if they'd be able to share the data. I unfortunately didn't have time before the deadline to try asking the VSA anyway.

The project I pursued was testing whether immigration, declines in the proportion of the population that is white-only, and/or rates of certain types of "deaths of despair" were correlated with higher vote shares for Trump. These three variables (and/or adjacent ideas) are all narratives that have been floated by pundits, and sometimes by scholars or other data journalists, as themes that might help explain Trump's appeal. As a result, I was interested in testing those narratives specifically on Maryland's election results. Due to time constraints, I was only able to finish testing those narratives against 2020's election results before this assignment's deadline. During winter break, I hope to complete a second set of analyses and see if my findings are the same using 2013-2016 data, too.

The original migration data was originally formatted so each row represents a year, and each column represents a county's (or statewide) net international migration for that year. While reading in the data, I decided to pivot the dataframe so rows were counties and columns were years. This made it easier to work with and join with the rest of the data I was working with.

The visualizations in this notebook are a little bland because they're geared towards the statistical tests/checks I was doing (which was necessary to confirm if the potential story was "real" before pursuing it!) If any of my story pitches were accepted, my goal would be to expand my analysis on that specific topic and create interactive, story-focused visualizations and maps that allow readers to explore the data and the connections I found in Analysis 1 and/or 2.

The three story ideas I would pitch are:

- There's a narrative that part of Trump's appeal is a response/backlash to large amounts of immigration. However, areas experiencing higher levels of immigration voted for Trump at LOWER rates. Is this simply an effect of higher rates of immigrants settling in urban/suburban areas that already lean away from Trump? Or -- more interestingly -- is there an observable effect where greater exposure to immigrants actually decreases the likelihood that someone's voting behavior will reflect xenophobic attitudes? I would like to interview immigration and refugee settlement organizations to better understand how/where immigrants settle, why they settle there, and what input they have on community reactions to and integration with immigrant communities. I would also like to interview people with nativist/xenophobic political attitudes that live in areas with relatively small levels of immigration. I'm curious to see whether the real reason they're motivated to vote on that issue is because they're actually perceiving/reacting to increased immigration OUTSIDE their community and falsely believe it's occurring in theirs too. I would actually/genuinely be really interested in reporting on this.

- Counties experiencing larger declines in the proportion of their population that is white-only also voted against Trump at larger rates. Is this an effect of "white flight" or just changing demographics in an otherwise stable population? I'd like to find out if white citizens specifically are leaving the counties with the largest declines -- if so, where are they moving to? And are those locations seeing corresponding increases in their Republican/Trump vote shares? Of the white citizens that remain in regions experiencing huge declines in the white population, are their political views/values being influenced by those demographic changes? I would actually/genuinely be really interested in reporting on this one, too.

- Baltimore was a HUGE outlier in my overdose analysis -- why is that? Is it just because it's the only location in my data that isolates a standalone urban geography, and urban locations in general have similarly massive OD rates? Or does Baltimore's rate far surpass even other urban locations in MD (or the wider region)? This obviously isn't election-related, but it stands out to me as the most interesting finding in an analysis that otherwise didn't have the most interesting or strongest findings, at least statistically.

Data Sources:

- Migration data: https://opendata.maryland.gov/Demographic/Maryland-International-Migration-2001-2021/hq27-cfrc

- 2020 Election results: https://docs.google.com/spreadsheets/d/1oSzr7O14vzRxeethgAnugADhgHhZG4xX1DOh1ElmCNc/edit?usp=sharing
I manually copied and pasted this data into a google sheet and exported to csv, originally from: https://elections.maryland.gov/elections/2020/results/general/gen_detail_results_p2020_4_BOT001-.html)


- Population data: ACS data accessed via tidyverse

- OD data: extracted using tabula from: https://health.maryland.gov/vsa/Documents/Overdose/Annual_2020_Drug_Intox_Report.pdf
(Page 61 in pdf file, labeled as page 58 in the report.) Includes alcohol and drug related deaths. After extacting with tabula, I used google sheets to move/align the county/subdivision names with the rest of the data before exporting to csv. No other changes were made outside of R.


# Analysis 1:

Use the "Maryland International Migration: 2001-2021" data set in combination with 2020 election results to see whether regions experiencing higher rates of migration are positively correlated with a vote swing towards Donald Trump in 2020.

Since most of the other data I would be working with had MD's counties as rows rather than columns, I needed to use pivot_longer() and chatGPT's explanation of how the function works to transform my data set.


```{r}
# Load the Maryland Migration data


migration_df <- read_csv("data/Maryland_International_Migration__2001-2021.csv")

migration_df_transformed <- migration_df %>%
  select(-`Date created`) %>%
  pivot_longer(cols = !Year,
               names_to = "county",
               values_to = "migration")
```
Next, I wanted to make a df showing total migration from 2017-2020 per county. I also saw that some of the other data I would be working with referred to statewide totals as "State Level," so I updated the name in this df accordingly

```{r}

migration_17_20 <- migration_df_transformed %>%
  filter(Year >= 2017 & Year <= 2020) %>%
  group_by(county) %>%
  summarize(avg_migration = sum(migration) / 4) %>%
  mutate(county = str_replace(county, " County", ""))

#ChatGPT helped with this line
migration_17_20$county[migration_17_20$county == "MARYLAND"] <- "State Level"

migration_17_20
```

Next, I needed to load 2020 election data and ensured that the rows/columns in the df would allow me to join with the migration data later on. I also came back to this step later on to crate a trump_won boolean column, which would later allow me to compare migration levels in counties that Trump won vs ones he lost.

```{r}
# Load 2020 election data

md_2020_county_election_results <- read_csv("data/md_2020_county_election_results.csv") %>%
  clean_names() %>%
  #chatGPT helped me with the next few lines to calculate the total votes cast in each county via a row-wise sum 
  rowwise() %>%
  mutate(total_county_votes = sum(c_across(-1), na.rm = TRUE)) %>%
  select(jurisdiction, donald_j_trump_and_michael_pence_republican, joe_biden_and_kamala_harris_democratic, total_county_votes)

# Create a new row with "State Level" in the "jurisdiction" column
# and the sum of the second column
new_row <- data.frame(jurisdiction = "State Level", donald_j_trump_and_michael_pence_republican = sum(md_2020_county_election_results$donald_j_trump_and_michael_pence_republican), joe_biden_and_kamala_harris_democratic = sum(md_2020_county_election_results$joe_biden_and_kamala_harris_democratic), total_county_votes = sum(md_2020_county_election_results$total_county_votes))

# Bind the new row to the original dataframe
md_2020_county_election_results <- rbind(md_2020_county_election_results, new_row)

# Create a boolean column that indicates whether Trump won the county or not

md_2020_county_election_results <- md_2020_county_election_results %>%
  # for some reason I was running into a "recycling" error when trying to create the boolean column straight away using the comparison statement. The error disappeared if I first created the column by making all the values 0, and then updated the values using the comparison statement. I don't get it, but at least it works now.
  mutate(trump_won = 0) %>%
  mutate(trump_won = donald_j_trump_and_michael_pence_republican > joe_biden_and_kamala_harris_democratic)

md_2020_county_election_results

```

Time to join: 

```{r}
# join the migration data

md_2020_election_and_migration <- migration_17_20 %>%
  left_join(md_2020_county_election_results, join_by("county" == "jurisdiction"))


md_2020_election_and_migration
```

These counties have drastically different populations, so I needed to incorporate ACS data to get total population numbers and calculate per capita migration levels, which would allow me to compare migration rates across counties.

```{r}
# load ACS data for 2020 (variable: B01003_001)

population_2020 <- get_acs(geography = "county",
              variables = "B01003_001",
              state = "MD",
              year = 2020) %>%
  mutate("NAME" = gsub(", Maryland", "", NAME)) %>%
      arrange(estimate) %>%
  select(GEOID, NAME, estimate)

new_row <- population_2020 %>%
  summarize(GEOID = "24", NAME = "State Level", estimate = sum(estimate))

population_2020 <- bind_rows(population_2020, new_row) %>%
  mutate(NAME = str_replace(NAME, " County", "")) %>%
  mutate(NAME = str_replace(NAME, "city", "City"))

population_2020

```

Join with our migration and election data

```{r}
# join with the migration and election data

full_2020_df <- md_2020_election_and_migration %>%
  left_join(population_2020, join_by("county" == "NAME"))

full_2020_df

```

Calculating Trumps vote share so we can do better statistical tests later with continuous variables rather than just the categorical variable of trump_won

```{r}
# calculate Trump's vote share %

full_2020_df <- full_2020_df %>%
  mutate(trump_vote_share = donald_j_trump_and_michael_pence_republican / total_county_votes * 100) %>%
  mutate(migration_per_capita = avg_migration / estimate * 10000) %>%
  filter(county != "State Level")

full_2020_df
```

The data is ready to use now! Let's start our analysis by grouping the data to compare average migration per capita in counties that Trump won vs counties he lost.

```{r}

full_2020_df %>%
  group_by(trump_won) %>%
  summarise(avg_migration_across_counties = sum(migration_per_capita) / n(),
            number_of_counties = n())

```

Average migration was MUCH higher in counties that Trump lost. That being said, we can do better than this in terms of statistical rigor. Let's calculate the correlation between per capita migration and Trump's vote share in a county.


First, plot the data to see if any Pearson assumptions are violated:

```{r}
ggplot(full_2020_df, aes(x = trump_vote_share, y = migration_per_capita)) +
  geom_point()
```


There's heteroscedasticity. My data also isn't normally distributed -- let's visualize that with a histogram to illustrate the extent of the problem:

```{r}
# Create a histogram
ggplot(full_2020_df, aes(x = migration_per_capita)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  labs(title = "The distribution of migration data is right skewed",
       x = "Migration Per Capita",
       y = "Frequency")

# Summary statistics
summary(full_2020_df$migration_per_capita)
```

This means we can't use Pearson's to calculate correlation. Let's use Kendall's Tau instead since it doesn't rely on an assumption of normally distributed data or homoscedasticity, and is also better with small sample sizes than Spearman. (ChatGPT showed me the R syntax for this)

```{r}

# Kendall's Tau correlation test
cor_test_result_kendall <- cor.test(full_2020_df$migration_per_capita, full_2020_df$trump_vote_share, method = "kendall")

# Print the result
print(cor_test_result_kendall)

```

Let's redo the scatterplot now, this time adding a non-linear line and switching the x and y axis to better illustrate the relationship/findings we're describing between these two variables

```{r}
# Scatterplot with smooth line
ggplot(full_2020_df, aes(x = migration_per_capita, y = trump_vote_share)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +  # Add a smooth line
  labs(
    title = "Counties experiencing more migration less likely to vote for Trump",
    x = "Annual international migration per capita (2017-2020 average)",
    y = "Trump's 2020 vote share (by county)"
  ) +
  theme(
    axis.title.x = element_text(size = 12),  # Adjust the size if needed
    axis.title.y = element_text(size = 12)   # Adjust the size if needed
  )
```

FINDINGS: There's a moderately negative correlation between a county's per capita migration numbers and Trump's vote share in that county. There's also a strong P value. In other words, we can say with a good amount of certainty that a county in MD experiencing higher levels of immigration is somewhat LESS likely to vote for Trump. More on this can be found in the "SUmmary" section at the top of this notebook.


# Analysis 2:

Check whether zips or state legislative districts that experienced a decline in the white population between 2017-2020 were more likely to vote for Trump, since that has been another narrative/possible explanation for his appeal to some voters (racial resentment toward a demographically changing nation/community).

NOTE: this is a minor adjustment from my original plan to measure change in white pop using zcta or legislative district-level data. I've made the change after getting permission from Derek because Kendall's Tau allowed me to perform reliable statistical tests with small sample sizes in Analysis 1. As a result, we're going to keep Analysis 2 and 3 at the county level so we're using the same geography across all three analyses.




First, we're going to need two ACS variables to make the calculations we need: the number of white-only citizens per county, and the total population per county. This will allow us to calculate how many white citizens there are per capita in each county.

We're going to be calculating % change in the proportion of each county's population that's white from 2017 to 2020, which means we're going to need the numbers I described above for both 2017 AND 2020.

Let's do 2017 first:

```{r}
white_and_total_pop_2017 <- get_acs(geography = "county",
              variables = c("B01003_001", "B02001_002"),
              state = "MD",
              year = 2017)

white_and_total_pop_2017
```

The format that the data came in from tidycensus using two variables isn't great. We could probably use group_by/summarise to reformat the data, but now that I know about pivot_longer and pivot_wider, let's use pivot_wider to transform the data so that we have a separate "estimate" column for each variable and only one row per county.

```{r}
white_and_total_pop_2017_transformed <- white_and_total_pop_2017 %>%
  select(-moe) %>%
  pivot_wider(
    names_from = variable,
    values_from = estimate
    ) %>%
  rename(total_pop_2017 = B01003_001, white_only_pop_2017 = B02001_002)

white_and_total_pop_2017_transformed

```
We eed to update the county names in this data so they'll join with our outher data sets later on.

```{r}
full_pop_2017 <- white_and_total_pop_2017_transformed %>%
  mutate(white_only_per_capita_2017 = white_only_pop_2017 / total_pop_2017 * 10000) %>%
  mutate("NAME" = gsub(", Maryland", "", NAME)) %>%
      arrange(white_only_per_capita_2017) %>%
  mutate(NAME = str_replace(NAME, " County", "")) %>%
  mutate(NAME = str_replace(NAME, "city", "City"))

full_pop_2017
```


Now repeat the above steps with 2020 ACS data

```{r}

white_and_total_pop_2020 <- get_acs(geography = "county",
              variables = c("B01003_001", "B02001_002"),
              state = "MD",
              year = 2020)

white_and_total_pop_2020


white_and_total_pop_2020_transformed <- white_and_total_pop_2020 %>%
  select(-moe) %>%
  pivot_wider(
    names_from = variable,
    values_from = estimate
    ) %>%
  rename(total_pop_2020 = B01003_001, white_only_pop_2020 = B02001_002)

full_pop_2020 <- white_and_total_pop_2020_transformed %>%
  mutate(white_only_per_capita_2020 = white_only_pop_2020 / total_pop_2020 * 10000) %>%
  mutate("NAME" = gsub(", Maryland", "", NAME)) %>%
      arrange(white_only_per_capita_2020) %>%
  mutate(NAME = str_replace(NAME, " County", "")) %>%
  mutate(NAME = str_replace(NAME, "city", "City"))

full_pop_2020
```

Join the 2017 and 2020 data and create a column that calculates % change in the white-only population in each county between 2017-2020

```{r}

full_pop_17_20 <- full_pop_2017 %>%
  inner_join(full_pop_2020) %>%
  mutate(white_only_per_cap_percent_change_vs_2017 = (white_only_per_capita_2020 - white_only_per_capita_2017) / white_only_per_capita_2017 * 100) %>%
  select(GEOID, NAME, white_only_per_cap_percent_change_vs_2017)

full_pop_17_20

```

Let's join this with our migration and election data

```{r}

full_2020_df <- full_2020_df %>%
  inner_join(full_pop_17_20)

full_2020_df

```

group by whether Trump won or lost and check the average % change in white populations in those two groups of counties.

```{r}
full_2020_df %>%
  group_by(trump_won) %>%
  summarise(avg_white_only_per_cap_percent_change_vs_2017 = sum(white_only_per_cap_percent_change_vs_2017) / n(),
            number_of_counties = n())
```

Counties that Trump lost had LARGER DECLINES in their white population compared to the counties he won.

Time to check the data before calculating correlation using Trump's vote share rather than just win/lose

```{r}

ggplot(full_2020_df, aes(x = white_only_per_cap_percent_change_vs_2017, y = trump_vote_share)) +
  geom_point() +
  # chatGPT told me how to do this next line to invert the axis
  scale_x_reverse()

```

Similar data issues as before so we're going to use the the same statistical tests

```{r}

# Kendall's Tau correlation test
cor_test_white_pop_trump_votes <- cor.test(full_2020_df$white_only_per_cap_percent_change_vs_2017, full_2020_df$trump_vote_share, method = "kendall")

# Print the result
print(cor_test_white_pop_trump_votes)

```

Generally, counties that experienced LARGER declines in the proportion of their population that is white, voted for Trump at LOWER rates. There's a solid correlation here and an awesome p-value. Let's visualize it. I chose to reverse the x axis to better illustrate the relationship in the same way I described it in this paragraph.

```{r}
# Scatterplot with smooth line
ggplot(full_2020_df, aes(x = white_only_per_cap_percent_change_vs_2017, y = trump_vote_share)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +  # Add a smooth line
  scale_x_reverse() +
  labs(
    title = "Counties with shrinking white populations less likely to vote for Trump",
    x = "Decline in white citizens per capita, 2017-2020 (%)",
    y = "Trump's 2020 vote share (by county)"
  ) +
  theme(
    axis.title.x = element_text(size = 12),  # Adjust the size if needed
    axis.title.y = element_text(size = 12)   # Adjust the size if needed
  )

```

# Analysis 3:

This will be a similar analysis as above, but using overdose rates as the independent variable instead of migration or the decrease of a region's white population. High rates of deaths of despair have been linked to higher vote shares for Trump in the past. For example: https://www.nbcnews.com/health/health-news/where-despair-deaths-were-higher-voters-chose-trump-n906631

Read in an examine the OD data

```{r}

od_df <- read_csv("data/od_deaths_per_year.csv")

od_df

```

The county names are going to need some cleaning. For the sake of time and efficiency I asked chatGPT to help with these steps after giving it some sample rows and describing what I wanted the output to be. The code it gave me needed to be modified since some of it didn't produce the desired output, but I still leaned heavily on it for these cleaning steps.


```{r}
#chatGPT helped me with these cleaning steps

# Clean up the values
od_df$SUBDIVISION <- gsub("\\.{3,}", "", od_df$SUBDIVISION)   # Remove extra periods
od_df$SUBDIVISION <- trimws(od_df$SUBDIVISION)                # Remove extra whitespaces
od_df$SUBDIVISION <- tolower(od_df$SUBDIVISION)               # Convert to lowercase
od_df$SUBDIVISION <- tools::toTitleCase(od_df$SUBDIVISION)    # Capitalize first letter of each word
od_df$SUBDIVISION <- gsub(" County", "", od_df$SUBDIVISION)   # Remove " County"
od_df$SUBDIVISION <- gsub("St Mary’s", "St. Mary's", od_df$SUBDIVISION)  # Add period after "St"

# Print the cleaned values
print(od_df$SUBDIVISION)

```

We need to get rid of the aggregate rows so they don't mess up our calculations later. I also wanted to update the column names so they would still be meaningful after joining this data with ful_df_2020 later on.

```{r}
od_df <- od_df %>%
  filter(SUBDIVISION != "Maryland" & SUBDIVISION != "Northwest Area" & SUBDIVISION != "Baltimore Metro Area" & SUBDIVISION != "National Capital Area" & SUBDIVISION != "Southern Area" & SUBDIVISION != "Eastern Shore Area") %>%
  rename(county = SUBDIVISION)

  #chatGPT helped with the next line
  names(od_df)[2:9] <- paste0("ods_", names(od_df)[2:9])

od_df
```

Narrow the data to 2017-2020 and calculate the average number of annual ODS across those four years

```{r}
ods_17_20 <- od_df %>%
  select(1, 6:9) %>%
  rowwise() %>%
  mutate(avg_ods_17_to_20 = sum(c_across(-1), na.rm = TRUE)/ 4)

ods_17_20
```

Let's get the ACS 2020 total population data ready so we can calculate per capita ODs

```{r}
total_pop_2020_df <- full_pop_2020 %>%
  select(GEOID, NAME, total_pop_2020)

total_pop_2020_df
```

Let's join it with the OD data:

```{r}

joined_pop_ods_17_20 <- ods_17_20 %>%
  inner_join(total_pop_2020_df, join_by("county" == "NAME"))

joined_pop_ods_17_20
```

... Something's gone wrong, I'm missing 2 counties. Let's do an anti_join to check what they are.


```{r}
unmatched_ods_17_20 <- ods_17_20 %>%
  anti_join(total_pop_2020_df, join_by("county" == "NAME"))

unmatched_ods_17_20
```

I'm seeing these exact values in both dfs. What's going on? Let's manually search for one of the two problematic values in each df

```{r}
ods_17_20 %>%
  filter(county == "Prince George’s")
```

```{r}
total_pop_2020_df %>%
  filter(NAME == "Prince George’s")
```

... I don't understand -- I can see this value in total_pop_2020_df but it's not being recognized. I cleaned extra whitespaces earlier so that's not the issue.

If the value I'm looking at isn't "Prince George’s" then what the heck is it? Using indexing 

```{r}
value <- total_pop_2020_df[1, 2]

value
```

I don't see or understand the difference between these values. Do a full join and try to spot the differences again

```{r}
joined_ods_17_20 <- ods_17_20 %>%
  full_join(total_pop_2020_df, join_by("county" == "NAME")) %>%
  arrange(county)

joined_ods_17_20
```

Are you kidding me... after a LOT of staring at what looked like identical values, I think the problem is that they use different types of apostrophes.

Check the encodings of the text in both dfs

```{r}
encodings <- sapply(total_pop_2020_df, function(NAME) stri_enc_mark(NAME))

print(encodings)
```

```{r}
encodings <- sapply(ods_17_20, function(county) stri_enc_mark(county))

print(encodings)
```
Some of the encodings here are utf-8 instead of ASCII

Updating the encoding in ods_17_20 to deal with this (chatGPT helped with these next steps)

```{r}
ods_17_20$county <- stri_trans_general(ods_17_20$county, "ASCII")
```

Check that it worked.

```{r}
encodings <- sapply(ods_17_20, function(county) stri_enc_mark(county))

print(encodings)
```


Let's try the join again now.

```{r}
joined_ods_17_20 <- ods_17_20 %>%
  inner_join(total_pop_2020_df, join_by("county" == "NAME")) %>%
  arrange(county)

joined_ods_17_20
```

Thank god and good riddance.

Calculate per capita ODs

```{r}
joined_ods_17_20 <- joined_ods_17_20 %>%
  mutate(ods_per_cap_2020 = avg_ods_17_to_20/total_pop_2020 * 10000)

joined_ods_17_20
```

join with the migration, race, and election data

```{r}

joined_ods_17_20 <- joined_ods_17_20 %>%
  select(county, GEOID, ods_per_cap_2020)

full_2020_df <- full_2020_df %>%
  inner_join(joined_ods_17_20)

full_2020_df

```

Compare OD rates in counties Trump won vs lost

```{r}
full_2020_df %>%
  group_by(trump_won) %>%
  summarise(avg_ods = sum(ods_per_cap_2020) / n(),
            number_of_counties = n())
```
There's virtually no difference here. Let's do our stat tests to check if that holds true using Trump's vote share rather than wins/losses

```{r}
ggplot(full_2020_df, aes(x = ods_per_cap_2020, y = trump_vote_share)) +
  geom_point()
```

```{r}
# Kendall's Tau correlation test
cor_test_ods_trump_votes <- cor.test(full_2020_df$ods_per_cap_2020, full_2020_df$trump_vote_share, method = "kendall")

# Print the result
print(cor_test_ods_trump_votes)
```
(note: haven't updated the labels, titles, etc on this viz since I decide not to use this one to determine findings -- see below)

```{r}
# Scatterplot with smooth line
ggplot(full_2020_df, aes(x = ods_per_cap_2020, y = trump_vote_share)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +  # Add a smooth line
  labs(title = paste("Kendall's Tau =", round(cor_test_ods_trump_votes$estimate, 3), "\n p-value =", cor_test_ods_trump_votes$p.value))
```

Baltimore City appears to be a massive outlier in this data due to the horrific rate of ODs there eclipsing rates anywhere else in the state. Not sure if this is statistically responsible or not especially given an already-small n, but let's see what happens if we remove it:

```{r}
full_2020_df_no_balt_city <- full_2020_df %>%
  filter(county != "Baltimore City")
```

Rerun the above steps and statistical tests

```{r}
full_2020_df_no_balt_city %>%
  group_by(trump_won) %>%
  summarise(avg_ods = sum(ods_per_cap_2020) / n(),
            number_of_counties = n())
```

Here there is a noticeable difference -- when you exclude Baltimore City, counties that Trump won had higher OD rates than counties he lost.

```{r}
# Kendall's Tau correlation test
cor_test_ods_trump_votes <- cor.test(full_2020_df_no_balt_city$ods_per_cap_2020, full_2020_df_no_balt_city$trump_vote_share, method = "kendall")

# Print the result
print(cor_test_white_pop_trump_votes)
```



```{r}
# Scatterplot with smooth line
ggplot(full_2020_df_no_balt_city, aes(x = ods_per_cap_2020, y = trump_vote_share)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +  # Add a smooth line
  labs(
    title = "OD rates only had small ties to Trump's vote share",
    x = "ODs per capita (2017-2020 averages)",
    y = "Trump's 2020 vote share (by county)"
  ) +
  theme(
    axis.title.x = element_text(size = 12),  # Adjust the size if needed
    axis.title.y = element_text(size = 12)   # Adjust the size if needed
  )

```

I'm surprised that the correlation wasn't a little bit stronger based on the appearance of the scatterplot. But there is only a small positive correlation between average ods per capita from 2017-2020 and Trump's vote share in MD counties in 2020. The p-value also barely passed muster.

