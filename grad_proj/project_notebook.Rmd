---
title: "JOUR772 Grad Assignment"
author: "Aidan Hughes"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidycensus)
library(janitor)
library(stringi)
```

# Summary of data

I've chosen to work with the Maryland International Migration data set from the open data portal. This data set covers 2001-2021 and includes county-level "foreign-born international migration, net movement to/ from Puerto Rico, net Armed Forces movement and native emigration." The source is described as being "from the Population Division, U.S. Census Bureau, March 2022" and was provided on the portal by the Maryland Department of Planning. I've put a call in to the DoP to get more information on the Census data they apparently used to create this aggregate data set so I can replicate and/or modify their calculations (eg I would probably take troop movement out of the mix for the purposes of this analysis), but I wasn't able to reach anybody before the deadline for this assignment.

I've combined that data set with three other county-level data sets pertaining to election results in MD, population/racial data from the ACS, and overdose data from the state department of health.

The project I pursued was testing whether immigration, declines in the proportion of the population that is white-only, and/or rates of certain types of "deaths of despair" were correlated with higher vote shares for Trump. These three variables (and/or adjacent ideas) are all narratives that have been floated by pundits, and sometimes by scholars or other data journalists, as themes that might help explain Trump's appeal. As a result, I was interested in testing those narratives specifically on Maryland's election results. Due to time constraints, I was only able to finish testing those narratives against 2020's election results before this assignment's deadline. Later on, I hope to see if my findings are the same using 2016 election results, too.

The original migration data was originally formatted so each row represents a year, and each column represents a county's (or statewide) net international migration for that year. While reading in the data, I decided to pivot the dataframe so columns were counties and columns were years. This made it easier to work with and join with the rest of the data I was working with.

The three story ideas I would pitch are:

- There's a narrative that part of Trump's appeal is a response/backlash to large amounts of immigration. However, areas experiencing higher levels of migration voted for Trump at LOWER rates. Is this simply an effect of higher rates of immigrants settling in urban/suburban areas that already lean away from Trump? Or -- more interestingly -- is there an observable effect where greater exposure to immigrants actually decreases the likelihood that someone's voting behavior will reflect xenophobic attitudes? I would like to interview immigration and refugee settlement organizations to better understand how/where immigrants settle, why they settle there, and what input they have on community reactions to and integration with imigrant communities.

- Counties experiencing larger declines in the proportion of their population that is white-only also voted against Trump at larger rates. Is this an effect of "white flight" or just changing demographics in an otherwise stable population? I'd like to find out if white citizens specifically are leaving the counties with the largest declines -- if so, where are they moving to? And are those locations seeing corresponding increases in their Republican/Trump vote shares?

- Baltimore was a HUGE outlier in my overdose analysis -- why is that? Is it just because it's the only location in my data that isolates a standalone urban geography, and urban locations in general have similar OD rates? Or does Baltimore's rate far surpass even other urban locations in MD? This obviously isn't election-related, but it stands out to me as the most interesting finding in an analysis that otherwise didn't have the most interesting or strongest findings, statistically.


# Analysis 1:

Use the "Maryland International Migration: 2001-2021" data set in combination with 2020 election results to see whether regions experiencing higher rates of migration are positively correlated with a vote swing towards Donald Trump in 2020

## Sources:

- Migration data: https://opendata.maryland.gov/Demographic/Maryland-International-Migration-2001-2021/hq27-cfrc

- 2020 Election results: https://docs.google.com/spreadsheets/d/1oSzr7O14vzRxeethgAnugADhgHhZG4xX1DOh1ElmCNc/edit?usp=sharing
I manually copied and pasted this data into a google sheet and exported to csv, originally from: https://elections.maryland.gov/elections/2020/results/general/gen_detail_results_p2020_4_BOT001-.html)

- Population data: ACS data accessed via tidyverse

- OD data: extracted using tabula from: https://health.maryland.gov/vsa/Documents/Overdose/Annual_2020_Drug_Intox_Report.pdf
(Page 61 in pdf file, labeled as page 58 in the report.) Includes alcohol and drug related deaths. After extacting with tabula, I used google sheets to move/align the county/subdivision names with the rest of the data before exporting to csv. No other changes were made outside of R.

```{r}
# Load the Maryland Migration data


migration_df <- read_csv("data/Maryland_International_Migration__2001-2021.csv")

migration_df_transformed <- migration_df %>%
  select(-`Date created`) %>%
  pivot_longer(cols = !Year,
               names_to = "county",
               values_to = "migration")
```

```{r}
#Make a df showing total migration from 2017-2020 per county

migration_17_20 <- migration_df_transformed %>%
  filter(Year >= 2017 & Year <= 2020) %>%
  group_by(county) %>%
  summarize(avg_migration = sum(migration) / 4) %>%
  mutate(county = str_replace(county, " County", ""))

#ChatGPT helped with this line
migration_17_20$county[migration_17_20$county == "MARYLAND"] <- "State Level"

migration_17_20
```


```{r}
# Load 2020 election data

md_2020_county_election_results <- read_csv("data/md_2020_county_election_results.csv") %>%
  clean_names() %>%
  #chatGPT helped me with the next four lines
  rowwise() %>%
  mutate(total_county_votes = sum(c_across(-1), na.rm = TRUE)) %>%
  select(jurisdiction, donald_j_trump_and_michael_pence_republican, total_county_votes)

# Create a new row with "State Level" in the "jurisdiction" column
# and the sum of the second column
new_row <- data.frame(jurisdiction = "State Level", donald_j_trump_and_michael_pence_republican = sum(md_2020_county_election_results$donald_j_trump_and_michael_pence_republican), total_county_votes = sum(md_2020_county_election_results$total_county_votes))

# Bind the new row to the original dataframe
md_2020_county_election_results <- rbind(md_2020_county_election_results, new_row)

md_2020_county_election_results

```


```{r}
# join the migration data

md_2020_election_and_migration <- migration_17_20 %>%
  left_join(md_2020_county_election_results, join_by("county" == "jurisdiction"))


md_2020_election_and_migration
```


```{r}
# load ACS data for 2020 (variable: B01003_001)

population_2020 <- get_acs(geography = "county",
              variables = "B01003_001",
              state = "MD",
              year = 2020) %>%
  mutate("NAME" = gsub(", Maryland", "", NAME)) %>%
      arrange(estimate) %>%
  select(GEOID, NAME, estimate)

new_row <- population_2020 %>%
  summarize(GEOID = "24", NAME = "State Level", estimate = sum(estimate))

population_2020 <- bind_rows(population_2020, new_row) %>%
  mutate(NAME = str_replace(NAME, " County", "")) %>%
  mutate(NAME = str_replace(NAME, "city", "City"))

population_2020

```

```{r}
# join with the migration and election data

full_2020_df <- md_2020_election_and_migration %>%
  left_join(population_2020, join_by("county" == "NAME"))

full_2020_df

```

```{r}
# calculate Trump's vote share %

full_2020_df <- full_2020_df %>%
  mutate(trump_vote_share = donald_j_trump_and_michael_pence_republican / total_county_votes * 100) %>%
  mutate(migration_per_capita = avg_migration / estimate * 10000) %>%
  filter(county != "State Level")

full_2020_df %>%
  select(trump_vote_share, migration_per_capita)
```

Plot the data to see if any Pearson assumptions are violated:

```{r}
ggplot(full_2020_df, aes(x = trump_vote_share, y = migration_per_capita)) +
  geom_point()
```


There's heteroscedasticity. My data also isn't normally distributed:

```{r}
# Create a histogram
ggplot(full_2020_df, aes(x = migration_per_capita)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Migration Per Capita",
       x = "Migration Per Capita",
       y = "Frequency")

# Summary statistics
summary(full_2020_df$migration_per_capita)
```

Let's use Kendall's Tau to test correlation since it doesn't rely on an assumption of normally distributed data or homoscedasticity, and is also better with small sample sizes than Spearman

```{r}

# Kendall's Tau correlation test
cor_test_result_kendall <- cor.test(full_2020_df$migration_per_capita, full_2020_df$trump_vote_share, method = "kendall")

# Print the result
print(cor_test_result_kendall)

```

```{r}
# Scatterplot with smooth line
ggplot(full_2020_df, aes(x = migration_per_capita, y = trump_vote_share)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +  # Add a smooth line
  labs(
    title = "Counties experiencing more migration less likely to vote for Trump",
    x = "Annual international migration per capita (2017-2020 average)",
    y = "Trump's 2020 vote share (by county)"
  ) +
  theme(
    axis.title.x = element_text(size = 12),  # Adjust the size if needed
    axis.title.y = element_text(size = 12)   # Adjust the size if needed
  )
```

There's a moderately negative correlation between a county's per capita migration numbers and Trump's vote share in that county. There's also a strong P value. In other words, we can say with a good amount of certainty that a county experiencing higher levels of immigration is somewhat LESS likely to vote for Trump.


# Analysis 2:

Check whether zips or state legislative districts that experienced a decline in the white population between 2017-2020 were more likely to vote for Trump, since that has been another narrative/possible explanation for his appeal to some voters (racial resentment toward a demographically changing nation/community).

NOTE: this is a minor adjustment from my original plan to measure change in white pop from 2010-2020. I've made the change for two reasons: 1) conceptual clarity: let's keep it to the years between presidential elections 2) since Kendall's Tau allowed me to perform reliable statistical tests with small sample sizes in Analysis 1, we're going to keep Analysis 2 and 3 at the county level so we're using the same geography across all three analyses.

```{r}
white_and_total_pop_2017 <- get_acs(geography = "county",
              variables = c("B01003_001", "B02001_002"),
              state = "MD",
              year = 2017)

white_and_total_pop_2017
```

```{r}
white_and_total_pop_2017_transformed <- white_and_total_pop_2017 %>%
  select(-moe) %>%
  pivot_wider(
    names_from = variable,
    values_from = estimate
    ) %>%
  rename(total_pop_2017 = B01003_001, white_only_pop_2017 = B02001_002)

full_pop_2017 <- white_and_total_pop_2017_transformed %>%
  mutate(white_only_per_capita_2017 = white_only_pop_2017 / total_pop_2017 * 10000) %>%
  mutate("NAME" = gsub(", Maryland", "", NAME)) %>%
      arrange(white_only_per_capita_2017) %>%
  mutate(NAME = str_replace(NAME, " County", "")) %>%
  mutate(NAME = str_replace(NAME, "city", "City"))

full_pop_2017
```



```{r}

white_and_total_pop_2020 <- get_acs(geography = "county",
              variables = c("B01003_001", "B02001_002"),
              state = "MD",
              year = 2020)

white_and_total_pop_2020

```



```{r}
white_and_total_pop_2020_transformed <- white_and_total_pop_2020 %>%
  select(-moe) %>%
  pivot_wider(
    names_from = variable,
    values_from = estimate
    ) %>%
  rename(total_pop_2020 = B01003_001, white_only_pop_2020 = B02001_002)

full_pop_2020 <- white_and_total_pop_2020_transformed %>%
  mutate(white_only_per_capita_2020 = white_only_pop_2020 / total_pop_2020 * 10000) %>%
  mutate("NAME" = gsub(", Maryland", "", NAME)) %>%
      arrange(white_only_per_capita_2020) %>%
  mutate(NAME = str_replace(NAME, " County", "")) %>%
  mutate(NAME = str_replace(NAME, "city", "City"))

full_pop_2020
```

Add a col to the 2020 data that calculates % change in the white-only population in each county between 2017-2020

```{r}

full_pop_17_20 <- full_pop_2017 %>%
  inner_join(full_pop_2020) %>%
  mutate(white_only_per_cap_percent_change_vs_2017 = (white_only_per_capita_2020 - white_only_per_capita_2017) / white_only_per_capita_2017 * 100) %>%
  select(GEOID, NAME, white_only_per_cap_percent_change_vs_2017)

full_pop_17_20

```

```{r}

full_2020_df <- full_2020_df %>%
  inner_join(full_pop_17_20)

full_2020_df

```

```{r}
full_2020_df$county
```


```{r}

ggplot(full_2020_df, aes(x = white_only_per_cap_percent_change_vs_2017, y = trump_vote_share)) +
  geom_point() +
  # chatGPT told me how to do this next line to invert the axis
  scale_x_reverse()

```

Same data issues as before so we're going to use the the same statistical tests

```{r}

# Kendall's Tau correlation test
cor_test_white_pop_trump_votes <- cor.test(full_2020_df$white_only_per_cap_percent_change_vs_2017, full_2020_df$trump_vote_share, method = "kendall")

# Print the result
print(cor_test_white_pop_trump_votes)

```

Generally, counties that experienced LARGER declines in the proportion of their population that is white, voted for Trump at SMALLER rates. There's a solid correlation here and an awesome p-value. Let's visualize it:

```{r}
# Scatterplot with smooth line
ggplot(full_2020_df, aes(x = white_only_per_cap_percent_change_vs_2017, y = trump_vote_share)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +  # Add a smooth line
  scale_x_reverse() +
  labs(
    title = "Counties with shrinking white populations less likely to vote for Trump",
    x = "Decline in white citizens per capita 2017-2020 (%)",
    y = "Trump's 2020 vote share (by county)"
  ) +
  theme(
    axis.title.x = element_text(size = 12),  # Adjust the size if needed
    axis.title.y = element_text(size = 12)   # Adjust the size if needed
  )

```

# Analysis 3:

Similar analysis but using overdose rates as the independent variable instead of migration or the decrease of a region's white population. High rates of deaths of despair have been linked to higher vote shares for Trump in the past. For example: https://www.nbcnews.com/health/health-news/where-despair-deaths-were-higher-voters-chose-trump-n906631

```{r}

od_df <- read_csv("data/od_deaths_per_year.csv")

od_df

```

```{r}
#chatGPT helped me with these cleaning steps

# Clean up the values
od_df$SUBDIVISION <- gsub("\\.{3,}", "", od_df$SUBDIVISION)   # Remove extra periods
od_df$SUBDIVISION <- trimws(od_df$SUBDIVISION)                # Remove extra whitespaces
od_df$SUBDIVISION <- tolower(od_df$SUBDIVISION)               # Convert to lowercase
od_df$SUBDIVISION <- tools::toTitleCase(od_df$SUBDIVISION)    # Capitalize first letter of each word
od_df$SUBDIVISION <- gsub(" County", "", od_df$SUBDIVISION)   # Remove " County"
od_df$SUBDIVISION <- gsub("St Mary’s", "St. Mary's", od_df$SUBDIVISION)  # Add period after "St"

# Print the cleaned values
print(od_df$SUBDIVISION)

```

```{r}
od_df <- od_df %>%
  filter(SUBDIVISION != "Maryland" & SUBDIVISION != "Northwest Area" & SUBDIVISION != "Baltimore Metro Area" & SUBDIVISION != "National Capital Area" & SUBDIVISION != "Southern Area" & SUBDIVISION != "Eastern Shore Area") %>%
  rename(county = SUBDIVISION)

  #chatGPT helped with the next line
  names(od_df)[2:9] <- paste0("ods_", names(od_df)[2:9])

od_df
```

```{r}
ods_13_16 <- od_df %>%
  select(1:5) %>%
  rowwise() %>%
  mutate(avg_ods_13_to_16 = sum(c_across(-1), na.rm = TRUE) / 4)

ods_13_16
```

```{r}
ods_17_20 <- od_df %>%
  select(1, 6:9) %>%
  rowwise() %>%
  mutate(avg_ods_17_to_20 = sum(c_across(-1), na.rm = TRUE)/ 4)

ods_17_20
```

```{r}
total_pop_2020_df <- full_pop_2020 %>%
  select(GEOID, NAME, total_pop_2020)

total_pop_2020_df
```

```{r}
unmatched_ods_17_20 <- ods_17_20 %>%
  anti_join(total_pop_2020_df, join_by("county" == "NAME"))

unmatched_ods_17_20
```

```{r}
ods_17_20 %>%
  filter(county == "Prince George’s")
```

```{r}
total_pop_2020_df %>%
  filter(NAME == "Prince George’s")
```

```{r}
value <- total_pop_2020_df[1, 2]

value
```

I don't see or understand the difference between these values. Do a full join and try to spot the differences again

```{r}
joined_ods_17_20 <- ods_17_20 %>%
  full_join(total_pop_2020_df, join_by("county" == "NAME")) %>%
  arrange(county)

joined_ods_17_20
```
Are you kidding me... the problem is that they use different types of apostrophes.

```{r}
encodings <- sapply(total_pop_2020_df, function(NAME) stri_enc_mark(NAME))

print(encodings)
```
```{r}
encodings <- sapply(ods_17_20, function(county) stri_enc_mark(county))

print(encodings)
```
Some of the encodings here are utf-8 instead of ASCII

Updating the encoding in ods_17_20 to deal with this

```{r}
ods_17_20$county <- stri_trans_general(ods_17_20$county, "ASCII")
```

```{r}
encodings <- sapply(ods_17_20, function(county) stri_enc_mark(county))

print(encodings)
```


Let's try again.

```{r}
joined_ods_17_20 <- ods_17_20 %>%
  inner_join(total_pop_2020_df, join_by("county" == "NAME")) %>%
  arrange(county)

joined_ods_17_20
```

Thank god and good riddance.

Calculate per capita ods

```{r}
joined_ods_17_20 <- joined_ods_17_20 %>%
  mutate(ods_per_cap_2020 = avg_ods_17_to_20/total_pop_2020 * 10000)

joined_ods_17_20
```

join with the election data

```{r}

joined_ods_17_20 <- joined_ods_17_20 %>%
  select(county, GEOID, ods_per_cap_2020)

full_2020_df <- full_2020_df %>%
  inner_join(joined_ods_17_20)

full_2020_df

```

```{r}
ggplot(full_2020_df, aes(x = ods_per_cap_2020, y = trump_vote_share)) +
  geom_point()
```

```{r}
# Kendall's Tau correlation test
cor_test_ods_trump_votes <- cor.test(full_2020_df$ods_per_cap_2020, full_2020_df$trump_vote_share, method = "kendall")

# Print the result
print(cor_test_ods_trump_votes)
```

```{r}
# Scatterplot with smooth line
ggplot(full_2020_df, aes(x = ods_per_cap_2020, y = trump_vote_share)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +  # Add a smooth line
  labs(title = paste("Kendall's Tau =", round(cor_test_ods_trump_votes$estimate, 3), "\n p-value =", cor_test_ods_trump_votes$p.value))
```

Baltimore City appears to be a massive outlier in this data due to the horrific rate of ODs there eclipsing rates anywhere else in the state. let's see what happens if we remove it:

```{r}
full_2020_df_no_balt_city <- full_2020_df %>%
  filter(county != "Baltimore City")
```

Rerun the above steps and statistical tests

```{r}
# Kendall's Tau correlation test
cor_test_ods_trump_votes <- cor.test(full_2020_df_no_balt_city$ods_per_cap_2020, full_2020_df_no_balt_city$trump_vote_share, method = "kendall")

# Print the result
print(cor_test_white_pop_trump_votes)
```

```{r}
# Scatterplot with smooth line
ggplot(full_2020_df_no_balt_city, aes(x = ods_per_cap_2020, y = trump_vote_share)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +  # Add a smooth line
  labs(
    title = "OD rates only had small ties to Trump's vote share",
    x = "ODs per capita (2017-2020 averages)",
    y = "Trump's 2020 vote share (by county)"
  ) +
  theme(
    axis.title.x = element_text(size = 12),  # Adjust the size if needed
    axis.title.y = element_text(size = 12)   # Adjust the size if needed
  )

```

I'm surprised that the correlation wasn't a little bit stronger based on the appearance of the scatterplot. But there is only a small positive correlation between average ods per capita from 2017-2020 and Trump's vote share in MD counties in 2020. The p-value also barely passed muster.


This is no longer election related, but from the data I collected, it seemed like ODs have been rising drastically year-by-year in most counties in Maryland. Let's group the full od data by year and plot it

```{r}
ods_13_16 <- od_df %>%
  select(1:5) %>%
  rowwise() %>%
  mutate(avg_ods_13_to_16 = sum(c_across(-1), na.rm = TRUE) / 4)

ods_13_16

```

```{r}
ods_17_20 <- od_df %>%
  select(1, 6:9) %>%
  rowwise() %>%
  mutate(avg_ods_17_to_20 = sum(c_across(-1), na.rm = TRUE)/ 4) %>%
  pivot_longer(cols = !Year,
               names_to = "county",
               values_to = "migration")

ods_17_20
```

```{r}
ods_13_20 <- rbind(ods_13_16, ods_17_20)
```

