---
title: "lab_05"
author: "derek willis"
date: "2023-10-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

-   Tabula

## Load libraries and establish settings

```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse, plus any other packages you will need to clean data and work with dates.
library(tidyverse)
library(lubridate)
library(janitor)

```

## Get Our PDF

We'll be working with the [911 overdose calls from Baltimore County](https://drive.google.com/file/d/1qkYuojGF_6WKFr5aNQxmewDzcKyOiJFr/view?usp=share_link). You'll want to download it to a place you'll remember (like your Downloads folder, or the labs folder in your repository). The goal is to extract the tables within it, export that to a CSV file, load it into RStudio and ask some questions.

## Extract Data from PDF Using Tabula

Start Tabula, then go to <http://127.0.0.1:8080/> in your browser. Click the "Browse" button and find the PDF file and click "open", and then click the "Import button" in Tabula. This will take a few seconds or longer.

This PDF has a single table spread over multiple pages to extract. We're going to make a single dataframe from this table, exporting it to a CSV file that you will load into R. In Tabula, highlight the table and click the "Preview & Export Extracted Data" button. You may want to play with including or excluding the column headers - YOU SHOULD HAVE FIVE COLUMNS OF DATA.

Save the CSV (it should be called `tabula-Baltimore County; Carey, Samantha log OD.csv` by default) to your lab_05/data folder.

From there, you will need to read in the data, and add or fix headers if necessary. You can choose to include the headers from the PDF in your exported CSV files OR to exclude them and add them when importing. `read_csv` allows us to do this ([and more](https://readr.tidyverse.org/reference/read_delim.html)).

## Load and clean up the data in R

You will need to read in and clean up the data so that it can be used for analysis. By "clean" I mean the column headers should not contain spaces and they should have meaningful names, not "x1" or something similar. How you do that is up to you, but you can use select() with or without the minus sign to include or exclude certain columns. You also can use the `rename` function to, well, rename columns. Importantly, you'll need to ensure that any columns containing a date actually have a date datatype. Our friend `lubridate` can help with this.

```{r}
baltimore_ods <- read_csv('updated_tabula-Baltimore County; Carey, Samantha log OD.csv', col_names = FALSE) |> 
  clean_names() |> 
  rename(call_date = x1, time = x2, case_nbr = x3, evtyp = x4, address = x5) %>%
  mutate(call_date = mdy(call_date))

baltimore_ods
```



## Answer questions

Q1. Write code to generate the number of calls that occurred on each date. Which date in 2022 had the most overdose calls, and how many? Look at the total number of rows in your result and explore the range of dates - based on your result, do you believe there are any days with no overdose calls at all? Explain why or why not.

A1.

# Which date in 2022 had the most overdose calls, and how many?

2022-07-14 and 2022-10-04	 had	23	calls

# Look at the total number of rows in your result and explore the range of dates - based on your result, do you believe there are any days with no overdose calls at all? Explain why or why not

I don't think so because there are 366 rows, which means there is a row for every day in a one-year time range

```{r}

# Write code to generate the number of calls that occurred on each date

baltimore_ods %>%
  group_by(call_date) %>%
  summarise(total_calls = n()) %>%
  arrange(desc(total_calls))


# Which date in 2022 had the most overdose calls, and how many?

# Look at the total number of rows in your result and explore the range of dates - based on your result, do you believe there are any days with no overdose calls at all?

# Explain why or why not

```


Q2. You want to understand if there's a pattern in the day of the week that overdose calls are made. Add a column to your dataframe that displays what day of the week each date represents. You should search for how to do that using lubridate. Then write code to calculate the number of calls for each day of the week, and add a column to that result that calculates the percentage of all calls that occurred on each day of the week (so you want a dataframe with the day of the week, total number of calls and the percentage of calls on that day out of the total number of all calls). Describe your findings to me.

A2.

Caley used chatgpt to find the wday function. Prompt: "I am working in R with the tidyverse library. I have a date column formatted like this: 2022-04-16. I want to create a new column that displays the day of the week for that date. Write me that code." We could have also updated the prompt by specifically asking it to give us a function from the lubridate package.

There is a slightly higher percentage of calls on Saturday and Sunday vs weekdays, but it's only a couple percentage points higher.

```{r}

# Add a column to your dataframe that displays what day of the week each date represents

day_baltimore_ods <- baltimore_ods %>%
  mutate(day_of_week = wday(call_date, label = TRUE, abbr = FALSE))



# Then write code to calculate the number of calls for each day of the week

day_baltimore_ods %>%
  group_by(day_of_week) %>%
  summarise(call_count = n()) %>%
  mutate(pct_of_calls = call_count / sum(call_count) * 100)

# and add a column to that result that calculates the percentage of all calls that occurred on each day of the week

```

Q3. Now let's look at locations. Which ones have the most calls? How would you describe them (feel free to search for more information on them)? Is there anything about the structure of the original data that might make you less confident in the counts by location or date?

A3.

4540 SILVER SPRING RD has	36 calls, vastly outnumbering any of the other results. We looked at google street view, which shows the address is adjacent to a major road and intersection. We're wondering if the address listed is not actually for the home itself at that address, but if it's instead a proxy for OD's that frequently occur on/near that road or intersection; in the park that's a bit behind the property based on Google maps; or something similar.

The next top two results are police precinct locations. We don't know if a) the data isn't accurate and these addresses represent - for example - the precinct that responded to a 911 call rather than the actual location of the call and/or incident, or b) if an overdose occurred for someone in police custody, and police had to request EMT from dispatch, and if that technically counted as a 911 call.

# Is there anything about the structure of the original data that might make you less confident in the counts by location or date?

Some locations, like some businesses (eg "TIMS MOTEL; 4415 WASHINGTON BL") have locations listed with both the name and the actual address of the location in the value. If the location appears elsewhere in the data without the business name in the value (eg "4415 WASHINGTON BL") then it would be grouped as a separate location, thereby affecting the call_count totals. This is indeed the case for the example location I mentioned above.


```{r}

day_baltimore_ods %>%
  group_by(address) %>%
  summarise(call_count = n()) %>%
  arrange(desc(call_count))

```

Q4. What's the best story idea or question you've seen as a result of the work you've done in this lab?

A4.

I would send myself or another reporter to 4540 SILVER SPRING RD and figure out what on earth is going on there. Is there a homeless encampment nearby? Is there access to a park or trail nearby? Could talk to neighbors at nearby addresses (and the homeowners of the address itself) to get a better idea of what's going on. Or did the people who live there really OD 36 times in a year?